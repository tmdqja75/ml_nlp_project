{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나는 너를 도와줄 예정이다 종료\n",
      "Exiting..\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'final_vectorizer.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [33], line 232\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m문장 확실성: \u001b[39m\u001b[38;5;124m\"\u001b[39m, cls_pred)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 232\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [33], line 223\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;66;03m# Now, put the transcription responses to use.\u001b[39;00m\n\u001b[0;32m    221\u001b[0m transcript \u001b[38;5;241m=\u001b[39m listen_print_loop(responses)\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m종료\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 223\u001b[0m type_pred, po_pred, tense_pred, cls_pred \u001b[38;5;241m=\u001b[39m \u001b[43mnlp_project\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtranscript\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m문장: \u001b[39m\u001b[38;5;124m\"\u001b[39m, transcript)\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m문장 유형: \u001b[39m\u001b[38;5;124m\"\u001b[39m, type_pred)\n",
      "Cell \u001b[1;32mIn [33], line 159\u001b[0m, in \u001b[0;36mnlp_project\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m    153\u001b[0m train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../ML_project/data/train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m#################          유형        ############################\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# 유형 분류 모델 불러오기\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m type_predict \u001b[38;5;241m=\u001b[39m \u001b[43msent_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype_ml_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m type_predict \u001b[38;5;241m=\u001b[39m type_predict[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    164\u001b[0m \u001b[38;5;66;03m##################         극성          ########################\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\ml_nlp_project\\type_sb\\sentence_type.py:24\u001b[0m, in \u001b[0;36mtype_ml_model\u001b[1;34m(sent)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtype_ml_model\u001b[39m(sent):\n\u001b[1;32m---> 24\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfinal_vectorizer.joblib\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     model \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_model.joblib\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m     sent_regex_filter \u001b[38;5;241m=\u001b[39m regex_filter(sent)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\ds_study\\lib\\site-packages\\joblib\\numpy_pickle.py:579\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    577\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 579\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[0;32m    581\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    582\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[0;32m    583\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[0;32m    584\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'final_vectorizer.joblib'"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from google.cloud import speech\n",
    "\n",
    "import pyaudio\n",
    "from six.moves import queue\n",
    "\n",
    "#환경변수 추가\n",
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.path.expanduser('../ML_project/dsstudy-368210-44c383232a36.json')\n",
    "\n",
    "# Audio recording parameters\n",
    "RATE = 16000\n",
    "CHUNK = int(RATE / 10)  # 100ms\n",
    "\n",
    "class MicrophoneStream(object):\n",
    "    \"\"\"Opens a recording stream as a generator yielding the audio chunks.\"\"\"\n",
    "\n",
    "    def __init__(self, rate, chunk):\n",
    "        self._rate = rate\n",
    "        self._chunk = chunk\n",
    "\n",
    "        # Create a thread-safe buffer of audio data\n",
    "        self._buff = queue.Queue()\n",
    "        self.closed = True\n",
    "\n",
    "    def __enter__(self):\n",
    "        self._audio_interface = pyaudio.PyAudio()\n",
    "        self._audio_stream = self._audio_interface.open(\n",
    "            format=pyaudio.paInt16,\n",
    "            # The API currently only supports 1-channel (mono) audio\n",
    "            # https://goo.gl/z757pE\n",
    "            channels=1,\n",
    "            rate=self._rate,\n",
    "            input=True,\n",
    "            frames_per_buffer=self._chunk,\n",
    "            # Run the audio stream asynchronously to fill the buffer object.\n",
    "            # This is necessary so that the input device's buffer doesn't\n",
    "            # overflow while the calling thread makes network requests, etc.\n",
    "            stream_callback=self._fill_buffer,\n",
    "        )\n",
    "\n",
    "        self.closed = False\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self._audio_stream.stop_stream()\n",
    "        self._audio_stream.close()\n",
    "        self.closed = True\n",
    "        # Signal the generator to terminate so that the client's\n",
    "        # streaming_recognize method will not block the process termination.\n",
    "        self._buff.put(None)\n",
    "        self._audio_interface.terminate()\n",
    "\n",
    "    def _fill_buffer(self, in_data, frame_count, time_info, status_flags):\n",
    "        \"\"\"Continuously collect data from the audio stream, into the buffer.\"\"\"\n",
    "        self._buff.put(in_data)\n",
    "        return None, pyaudio.paContinue\n",
    "\n",
    "    def generator(self):\n",
    "        while not self.closed:\n",
    "            # Use a blocking get() to ensure there's at least one chunk of\n",
    "            # data, and stop iteration if the chunk is None, indicating the\n",
    "            # end of the audio stream.\n",
    "            chunk = self._buff.get()\n",
    "            if chunk is None:\n",
    "                return\n",
    "            data = [chunk]\n",
    "\n",
    "            # Now consume whatever other data's still buffered.\n",
    "            while True:\n",
    "                try:\n",
    "                    chunk = self._buff.get(block=False)\n",
    "                    if chunk is None:\n",
    "                        return\n",
    "                    data.append(chunk)\n",
    "                except queue.Empty:\n",
    "                    break\n",
    "\n",
    "            yield b\"\".join(data)\n",
    "\n",
    "def listen_print_loop(responses):\n",
    "    \"\"\"Iterates through server responses and prints them.\n",
    "\n",
    "    The responses passed is a generator that will block until a response\n",
    "    is provided by the server.\n",
    "\n",
    "    Each response may contain multiple results, and each result may contain\n",
    "    multiple alternatives; for details, see https://goo.gl/tjCPAU.  Here we\n",
    "    print only the transcription for the top alternative of the top result.\n",
    "\n",
    "    In this case, responses are provided for interim results as well. If the\n",
    "    response is an interim one, print a line feed at the end of it, to allow\n",
    "    the next result to overwrite it, until the response is a final one. For the\n",
    "    final one, print a newline to preserve the finalized transcription.\n",
    "    \"\"\"\n",
    "    num_chars_printed = 0\n",
    "    for response in responses:\n",
    "        if not response.results:\n",
    "            continue\n",
    "\n",
    "        # The `results` list is consecutive. For streaming, we only care about\n",
    "        # the first result being considered, since once it's `is_final`, it\n",
    "        # moves on to considering the next utterance.\n",
    "        result = response.results[0]\n",
    "        if not result.alternatives:\n",
    "            continue\n",
    "\n",
    "        # Display the transcription of the top alternative.\n",
    "        transcript = result.alternatives[0].transcript\n",
    "\n",
    "\n",
    "        # Display interim results, but with a carriage return at the end of the\n",
    "        # line, so subsequent lines will overwrite them.\n",
    "        #\n",
    "        # If the previous result was longer than this one, we need to print\n",
    "        # some extra spaces to overwrite the previous result\n",
    "        overwrite_chars = \" \" * (num_chars_printed - len(transcript))\n",
    "\n",
    "        if not result.is_final:\n",
    "            sys.stdout.write(transcript + overwrite_chars + \"\\r\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            num_chars_printed = len(transcript)\n",
    "\n",
    "        else:\n",
    "            print(transcript + overwrite_chars)\n",
    "\n",
    "            # Exit recognition if any of the transcribed phrases could be\n",
    "            # one of our keywords.\n",
    "            if re.search(r\"\\b(종료|멈춤)\\b\", transcript, re.I):\n",
    "                print(\"Exiting..\")\n",
    "                \n",
    "                return transcript\n",
    "                \n",
    "                break\n",
    "                \n",
    "\n",
    "            num_chars_printed = 0\n",
    "        \n",
    "def nlp_project(sentence):\n",
    "    import time\n",
    "    import pandas as pd\n",
    "\n",
    "    import type_sb.sentence_type as sent_type\n",
    "    import type_sh.seokho as po\n",
    "    import tense_yeram.final_tense_ram as tense\n",
    "\n",
    "    train = pd.read_csv('../ML_project/data/train.csv')\n",
    "\n",
    "\n",
    "    #################          유형        ############################\n",
    "\n",
    "    # 유형 분류 모델 불러오기\n",
    "    type_predict = sent_type.type_ml_model(sentence)\n",
    "    type_predict = type_predict[0]\n",
    "\n",
    "\n",
    "\n",
    "    ##################         극성          ########################\n",
    "\n",
    "\n",
    "    po_model = po.getModel(train)\n",
    "\n",
    "    to_po_sentence = po.transformTarget(sentence, train)\n",
    "\n",
    "    # 0 긍정, 1 미정, 2 부정\n",
    "    po_pred = po_model.predict(to_po_sentence)\n",
    "\n",
    "    po_pred = \"긍정\" if po_pred[0] == 0 else \"미정\" if po_pred[0] == 1 else \"부정\"\n",
    "\n",
    "\n",
    "\n",
    "    ##################         시제          ########################\n",
    "\n",
    "    tense_pred = tense.tense_ml(sentence)\n",
    "    # result = '과거', '현재', '미래'\n",
    "\n",
    "\n",
    "\n",
    "    ##################         확실성           ########################\n",
    "    import sentence_classify as cls\n",
    "    import pandas as pd\n",
    "\n",
    "    vectorizer, model = cls.fit_model(train)\n",
    "    result_classify = cls.predict(sentence, vectorizer, model)\n",
    "    # result = \"확실\" or \"불확실\"\n",
    "    \n",
    "    return type_predict, po_pred, tense_pred, result_classify\n",
    "\n",
    "def main():\n",
    "    # See http://g.co/cloud/speech/docs/languages\n",
    "    # for a list of supported languages.\n",
    "    language_code = \"ko-KR\"  # a BCP-47 language tag\n",
    "\n",
    "    client = speech.SpeechClient()\n",
    "    config = speech.RecognitionConfig(\n",
    "        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
    "        sample_rate_hertz=RATE,\n",
    "        language_code=language_code,\n",
    "    )\n",
    "\n",
    "    streaming_config = speech.StreamingRecognitionConfig(\n",
    "        config=config, interim_results=True\n",
    "    )\n",
    "\n",
    "    with MicrophoneStream(RATE, CHUNK) as stream:\n",
    "        audio_generator = stream.generator()\n",
    "        requests = (\n",
    "            speech.StreamingRecognizeRequest(audio_content=content)\n",
    "            for content in audio_generator\n",
    "        )\n",
    "\n",
    "        responses = client.streaming_recognize(streaming_config, requests)\n",
    "\n",
    "        # Now, put the transcription responses to use.\n",
    "        transcript = listen_print_loop(responses).rstrip(\"종료\")\n",
    "        \n",
    "        type_pred, po_pred, tense_pred, cls_pred = nlp_project(transcript)\n",
    "        print(\"문장: \", transcript)\n",
    "        print(\"문장 유형: \", type_pred)\n",
    "        print(\"문장 극성: \", po_pred)\n",
    "        print(\"문장 시제: \", tense_pred)\n",
    "        print(\"문장 확실성: \", cls_pred)\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "189257d3ead7ed4cabfbed122a0ee4e0c566f64830e9ec3d7b4f7f74d108b2bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
